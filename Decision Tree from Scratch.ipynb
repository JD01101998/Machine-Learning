{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the dataset you want to predict the values for from the list:\n",
      "            1) IRIS dataset\n",
      "            2) Synthetic dataset\n",
      "            1\n",
      "\t\t is petal width (cm) >= 1.0\n",
      "\t\t --> True:\n",
      "\t\t   is petal width (cm) >= 1.8\n",
      "\t\t   --> True:\n",
      " \t\t     Predict {'virginica': 35}\n",
      "\t\t   --> False:\n",
      "\t\t     is petal length (cm) >= 5.8\n",
      "\t\t     --> True:\n",
      " \t\t       Predict {'virginica': 1}\n",
      "\t\t     --> False:\n",
      "\t\t       is sepal length (cm) >= 5.0\n",
      "\t\t       --> True:\n",
      "\t\t         is petal length (cm) >= 5.0\n",
      "\t\t         --> True:\n",
      "\t\t           is petal width (cm) >= 1.6\n",
      "\t\t           --> True:\n",
      " \t\t             Predict {'versicolor': 2}\n",
      "\t\t           --> False:\n",
      " \t\t             Predict {'virginica': 1}\n",
      "\t\t         --> False:\n",
      " \t\t           Predict {'versicolor': 34}\n",
      "\t\t       --> False:\n",
      " \t\t         Predict {'virginica': 1}\n",
      "\t\t --> False:\n",
      " \t\t   Predict {'setosa': 38}\n",
      "Full tree accuracy: [94.73684210526315] \n",
      "\t\t is petal width (cm) >= 1.0\n",
      "\t\t --> True:\n",
      "\t\t   is petal width (cm) >= 1.8\n",
      "\t\t   --> True:\n",
      " \t\t     Predict {'virginica': 35}\n",
      "\t\t   --> False:\n",
      "\t\t     is petal length (cm) >= 5.8\n",
      "\t\t     --> True:\n",
      " \t\t       Predict {'virginica': 1}\n",
      "\t\t     --> False:\n",
      "\t\t       is sepal length (cm) >= 5.0\n",
      "\t\t       --> True:\n",
      "\t\t         is petal length (cm) >= 5.0\n",
      "\t\t       --> False:\n",
      " \t\t         Predict {'virginica': 1}\n",
      "\t\t --> False:\n",
      " \t\t   Predict {'setosa': 38}\n",
      "Pruned tree accuracy: [94.73684210526315] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class DecisionTrees:\n",
    "\n",
    "    def __init__(self, split_condition=None, true_branch=None, false_branch=None, results={}, result=None, error=0):\n",
    "        self.split_condition = split_condition\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        self.results = results\n",
    "        self.result = result\n",
    "        self.error = error\n",
    "\n",
    "    @classmethod\n",
    "    def set_target_mapping(cls, mapping_dict, train_data):\n",
    "        cls.mapping_dict = mapping_dict\n",
    "        cls.train_data = train_data\n",
    "\n",
    "    @classmethod\n",
    "    def class_counts(cls, rows):\n",
    "        counts = dict()\n",
    "        for row in rows:\n",
    "            label = row[-1]\n",
    "            if cls.mapping_dict[label] not in counts:\n",
    "                counts[cls.mapping_dict[label]] = 0\n",
    "            counts[cls.mapping_dict[label]] += 1\n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def gini_index(cls, rows):\n",
    "        gini_index = 1\n",
    "        counts = cls.class_counts(rows)\n",
    "\n",
    "        for feature in counts:\n",
    "            probability = counts[feature] / float(len(rows))\n",
    "            gini_index -= probability ** 2\n",
    "        return gini_index\n",
    "\n",
    "    @classmethod\n",
    "    def information_gain(cls, left, right, parent_node_impurity):\n",
    "        p = float(len(left)) / (len(left) + len(right))\n",
    "        return parent_node_impurity - p * cls.gini_index(left) - (1 - p) * cls.gini_index(right)\n",
    "\n",
    "    @classmethod\n",
    "    def partition(cls, rows, col, val):\n",
    "        true_rows, false_rows = [], []\n",
    "        for row in rows:\n",
    "            if row[col] >= val:\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "\n",
    "    @classmethod\n",
    "    def split_data(cls, rows):\n",
    "        best_gain = 0\n",
    "        best_split = None\n",
    "        current_gini_index = cls.gini_index(rows)\n",
    "        n_features = len(rows[0]) - 1\n",
    "\n",
    "        for col in range(n_features):\n",
    "            values = set([row[col] for row in rows])\n",
    "            for val in values:\n",
    "                true_rows, false_rows = cls.partition(rows, col, val)\n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "                gain = cls.information_gain(true_rows, false_rows, current_gini_index)\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_split = gain, (col, val)\n",
    "        return best_gain, best_split\n",
    "\n",
    "    @classmethod\n",
    "    def build_tree(cls, rows):\n",
    "        results = cls.class_counts(rows)\n",
    "        result = sorted(results.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        error = 0\n",
    "        for k, v in results.items():\n",
    "            if k != result:\n",
    "                error += v\n",
    "        gain, split_condition = cls.split_data(rows)\n",
    "\n",
    "        if gain == 0:\n",
    "            return Leaf(rows, error)\n",
    "\n",
    "        true_rows, false_rows = cls.partition(rows, split_condition[0], split_condition[1])\n",
    "        true_branch = cls.build_tree(true_rows)\n",
    "        false_branch = cls.build_tree(false_rows)\n",
    "\n",
    "        return DecisionTrees(split_condition=split_condition, true_branch=true_branch, false_branch=false_branch, results=results,\n",
    "                             result=result,\n",
    "                             error=error)\n",
    "\n",
    "    def print_tree(self, node, column, spacing=\"\\t\\t\"):\n",
    "        if isinstance(node, Leaf):\n",
    "            print(f\" {spacing} Predict {node.predictions}\")\n",
    "            return\n",
    "\n",
    "        if node is not None:\n",
    "            print(f\"{spacing} is {column[node.split_condition[0]]} >= {node.split_condition[1]}\")\n",
    "\n",
    "            if node.true_branch is not None:\n",
    "                print(f\"{spacing} --> True:\")\n",
    "                self.print_tree(node.true_branch, column, spacing + \"  \")\n",
    "\n",
    "            if node.true_branch is not None:\n",
    "                print(f\"{spacing} --> False:\")\n",
    "                self.print_tree(node.false_branch, column, spacing + \"  \")\n",
    "\n",
    "    @classmethod\n",
    "    def accuracy_metric(cls, actual, predicted):\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if cls.mapping_dict[actual[i]] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) * 100.0\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_algorithm(cls, test_data):\n",
    "        scores = list()\n",
    "        test_set = list()\n",
    "        for row in test_data:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = cls.decision_tree(cls.train_data, test_set)\n",
    "        actual = [row[-1] for row in test_data]\n",
    "        predicted = [k for d in predicted for k in d]\n",
    "        accuracy = cls.accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "        return scores\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, node, row):\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.predictions\n",
    "\n",
    "        if row[node.split_condition[0]] >= node.split_condition[1]:\n",
    "            return cls.predict(node.true_branch, row)\n",
    "        else:\n",
    "            return cls.predict(node.false_branch, row)\n",
    "\n",
    "    @classmethod\n",
    "    def decision_tree(cls, train, test):\n",
    "        tree = cls.build_tree(train)\n",
    "        predictions = list()\n",
    "        for row in test:\n",
    "            prediction = cls.predict(tree, row)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    @classmethod\n",
    "    def minimum_error_pruning(cls, node, num_classes):\n",
    "        if isinstance(node, Leaf):\n",
    "            sum_ = list(node.predictions.values())[0]\n",
    "            error_leaf = (node.error + num_classes - 1) / (sum_ + num_classes)\n",
    "            return sum_, error_leaf\n",
    "\n",
    "        sum_ = sum(node.results.values())\n",
    "        error_leaf = (node.error + num_classes - 1) / (sum_ + num_classes)\n",
    "\n",
    "        sum_true, error_true = cls.minimum_error_pruning(node.true_branch, num_classes)\n",
    "        sum_false, error_false = cls.minimum_error_pruning(node.false_branch, num_classes)\n",
    "        error_subtree = sum_true / sum_ * error_true + sum_false / sum_ * error_false\n",
    "\n",
    "        if error_leaf <= error_subtree:\n",
    "            node.true_branch = None\n",
    "            node.false_branch = None\n",
    "            return sum_, error_leaf\n",
    "        return sum_, error_subtree\n",
    "\n",
    "    @classmethod\n",
    "    def plot_boundary(cls, X, y, node):\n",
    "        n_classes = 2\n",
    "        plot_colors = \"ryb\"\n",
    "        plot_step = 0.02\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))\n",
    "        p = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "        z = []\n",
    "        for row in p:\n",
    "            k = cls.predict(node, row)\n",
    "            z.append(int(list(k.keys())[0]))\n",
    "        z = np.array(z)\n",
    "        z = z.reshape(xx.shape)\n",
    "        cs = plt.contourf(xx, yy, z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "        plt.xlabel(\"X1\")\n",
    "        plt.ylabel(\"X2\")\n",
    "        a = ['X1', 'X2']\n",
    "        for i, color in zip(range(n_classes), plot_colors):\n",
    "            idx = np.where(y == i)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], label=a[i], cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "        plt.suptitle(\"Decision boundary for synthetic dataset\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, rows, error=None, cnt=0):\n",
    "        self.predictions = DecisionTrees.class_counts(rows)\n",
    "        self.cnt = cnt\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "def main():\n",
    "    option = int(input(f\"\"\"Input the dataset you want to predict the values for from the list:\n",
    "            1) IRIS dataset\n",
    "            2) Synthetic dataset\n",
    "            \"\"\"))\n",
    "    if option == 1:\n",
    "        data, target = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "        mapping_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "        num_classes = target.nunique()\n",
    "\n",
    "    elif option == 2:\n",
    "        df = pd.read_csv(\"dt_data.csv\")\n",
    "        data = df[[\"X1\", \"X2\"]]\n",
    "        target = df[[\"Y\"]]\n",
    "        mapping_dict = {0: 0, 1: 1}\n",
    "        num_classes = target.nunique().item()\n",
    "\n",
    "    else:\n",
    "        print(\"Please select a valid dataset from the list\")\n",
    "    column = data.columns\n",
    "    dataset = pd.concat([data, target], axis=1)\n",
    "    l = dataset.values.tolist()\n",
    "\n",
    "    train_data, test_data = train_test_split(l, random_state=5)\n",
    "\n",
    "    DecisionTrees.set_target_mapping(mapping_dict, train_data)\n",
    "    reg = DecisionTrees()\n",
    "    my_tree = reg.build_tree(train_data)\n",
    "\n",
    "    # Print and output score for full tree\n",
    "    reg.print_tree(my_tree, column)\n",
    "    accuracy = reg.evaluate_algorithm(test_data)\n",
    "    print(f\"Full tree accuracy: {accuracy} \")\n",
    "\n",
    "    reg.minimum_error_pruning(my_tree, num_classes)\n",
    "\n",
    "    # Print and output score for Pruned tree\n",
    "    reg.print_tree(my_tree, column)\n",
    "    accuracy = reg.evaluate_algorithm(test_data)\n",
    "    print(f\"Pruned tree accuracy: {accuracy} \")\n",
    "\n",
    "    # Decision Boundary plotting for synthetic dataset\n",
    "    if option == 2:\n",
    "        dt = reg.build_tree(l)\n",
    "        reg.plot_boundary(data, target, dt)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
